{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd3adbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2f105",
   "metadata": {},
   "source": [
    "__Embeddings__ map any word onto a vectorial representation (each word with a vector). For instance, the word dog can be represented by the vector (w1, w2,..., wn) in the embedding space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84bc3f0",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a6be73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 18:31:33.118777: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-08 18:31:33.507373: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-03-08 18:31:33.507392: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-03-08 18:31:33.568956: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-08 18:31:34.602301: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-08 18:31:34.602381: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-08 18:31:34.602386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2024-03-08 18:31:40.181931: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 80.23 MiB (download: 80.23 MiB, generated: Unknown size, total: 80.23 MiB) to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80b47455b4824e5eba523e4ccfc598d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae0c0d3882549e48a7e035471bd547b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteXWP62Z/imdb_reviews-train.tfrecord*...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteXWP62Z/imdb_reviews-test.tfrecord*...:…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0.incompleteXWP62Z/imdb_reviews-unsupervised.tfrec…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset imdb_reviews downloaded and prepared to ~/tensorflow_datasets/imdb_reviews/plain_text/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 18:32:09.601514: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-03-08 18:32:09.602056: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-03-08 18:32:09.602080: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (PORT-EMONTEIRO): /proc/driver/nvidia/version does not exist\n",
      "2024-03-08 18:32:09.603590: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "###########################################\n",
    "### Just run this cell to load the data ###\n",
    "###########################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f8d476c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['this', 'was', 'an', 'absolutely', 'terrible', 'movie', \"don't\", 'be', 'lured', 'in', 'by', 'christopher', 'walken', 'or', 'michael', 'ironside', 'both', 'are', 'great', 'actors', 'but', 'this', 'must', 'simply', 'be', 'their', 'worst', 'role', 'in', 'history', 'even', 'their', 'great', 'acting', 'could', 'not', 'redeem', 'this', \"movie's\", 'ridiculous', 'storyline', 'this', 'movie', 'is', 'an', 'early', 'nineties', 'us', 'propaganda', 'piece', 'the', 'most', 'pathetic', 'scenes', 'were', 'those', 'when', 'the', 'columbian', 'rebels', 'were', 'making', 'their', 'cases', 'for', 'revolutions', 'maria', 'conchita', 'alonso', 'appeared', 'phony', 'and', 'her', 'pseudo', 'love', 'affair', 'with', 'walken', 'was', 'nothing', 'but', 'a', 'pathetic', 'emotional', 'plug', 'in', 'a', 'movie', 'that', 'was', 'devoid', 'of', 'any', 'real', 'meaning', 'i', 'am', 'disappointed', 'that', 'there', 'are', 'movies', 'like', 'this', 'ruining', \"actor's\", 'like', 'christopher', \"walken's\", 'good', 'name', 'i', 'could', 'barely', 'sit', 'through', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(y_train[0], X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce1d1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['i', 'really', 'love', 'the', 'sexy', 'action', 'and', 'sci', 'fi', 'films', 'of', 'the', 'sixties', 'and', 'its', 'because', 'of', 'the', \"actress's\", 'that', 'appeared', 'in', 'them', 'they', 'found', 'the', 'sexiest', 'women', 'to', 'be', 'in', 'these', 'films', 'and', 'it', \"didn't\", 'matter', 'if', 'they', 'could', 'act', 'remember', 'candy', 'the', 'reason', 'i', 'was', 'disappointed', 'by', 'this', 'film', 'was', 'because', 'it', \"wasn't\", 'nostalgic', 'enough', 'the', 'story', 'here', 'has', 'a', 'european', 'sci', 'fi', 'film', 'called', 'dragonfly', 'being', 'made', 'and', 'the', 'director', 'is', 'fired', 'so', 'the', 'producers', 'decide', 'to', 'let', 'a', 'young', 'aspiring', 'filmmaker', 'jeremy', 'davies', 'to', 'complete', 'the', 'picture', \"they're\", 'is', 'one', 'real', 'beautiful', 'woman', 'in', 'the', 'film', 'who', 'plays', 'dragonfly', 'but', \"she's\", 'barely', 'in', 'it', 'film', 'is', 'written', 'and', 'directed', 'by', 'roman', 'coppola', 'who', 'uses', 'some', 'of', 'his', 'fathers', 'exploits', 'from', 'his', 'early', 'days', 'and', 'puts', 'it', 'into', 'the', 'script', 'i', 'wish', 'the', 'film', 'could', 'have', 'been', 'an', 'homage', 'to', 'those', 'early', 'films', 'they', 'could', 'have', 'lots', 'of', 'cameos', 'by', 'actors', 'who', 'appeared', 'in', 'them', 'there', 'is', 'one', 'actor', 'in', 'this', 'film', 'who', 'was', 'popular', 'from', 'the', 'sixties', 'and', 'its', 'john', 'phillip', 'law', 'barbarella', 'gerard', 'depardieu', 'giancarlo', 'giannini', 'and', 'dean', 'stockwell', 'appear', 'as', 'well', 'i', 'guess', \"i'm\", 'going', 'to', 'have', 'to', 'continue', 'waiting', 'for', 'a', 'director', 'to', 'make', 'a', 'good', 'homage', 'to', 'the', 'films', 'of', 'the', 'sixties', 'if', 'any', 'are', 'reading', 'this', 'make', 'it', 'as', 'sexy', 'as', 'you', 'can', \"i'll\", 'be', 'waiting']\n"
     ]
    }
   ],
   "source": [
    "print(y_train[8], X_train[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9621d28",
   "metadata": {},
   "source": [
    "__LABELS__: the task is a binary classification problem:\n",
    "\n",
    "- label __0__ corresponds to a negative movie review\n",
    "- label __1__ corresponds to a positive movie review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04503a6",
   "metadata": {},
   "source": [
    "__INPUTS__:\n",
    "\n",
    "- data has been partially cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522a5f8a",
   "metadata": {},
   "source": [
    "# Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "972f373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# This initializes a Keras utilities that does all the tokenization for you\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# The tokenization learns a dictionary that maps a token (integer) to each word\n",
    "# It can be done only on the train set - we are not supposed to know the test set!\n",
    "# This tokenization also lowercases your words, apply some filters, and so on - you can check the doc if you want\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "# We apply the tokenization to the train and test set\n",
    "X_train_token = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_token = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34ab4d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: i -> Token 9\n",
      "Word: enjoyed -> Token 579\n",
      "Word: this -> Token 11\n",
      "Word: movie -> Token 18\n",
      "Word: and -> Token 3\n",
      "Word: after -> Token 104\n",
      "Word: watching -> Token 155\n",
      "Word: it -> Token 10\n",
      "Word: it -> Token 10\n",
      "Word: made -> Token 90\n",
      "Word: me -> Token 65\n",
      "Word: wonder -> Token 574\n",
      "Word: just -> Token 40\n",
      "Word: how -> Token 84\n",
      "Word: many -> Token 107\n",
      "Word: 'caitlin -> Token 17238\n",
      "Word: rose's' -> Token 17239\n",
      "Word: exist -> Token 1632\n",
      "Word: in -> Token 8\n",
      "Word: the -> Token 1\n",
      "Word: world -> Token 189\n",
      "Word: how -> Token 84\n",
      "Word: many -> Token 107\n",
      "Word: other -> Token 82\n",
      "Word: girls -> Token 526\n",
      "Word: have -> Token 25\n",
      "Word: been -> Token 76\n",
      "Word: subjected -> Token 5574\n",
      "Word: to -> Token 5\n",
      "Word: this -> Token 11\n",
      "Word: sort -> Token 406\n",
      "Word: of -> Token 4\n",
      "Word: sexual -> Token 991\n",
      "Word: abuse -> Token 2393\n",
      "Word: and -> Token 3\n",
      "Word: torment -> Token 8200\n",
      "Word: by -> Token 31\n",
      "Word: classmates -> Token 7069\n",
      "Word: and -> Token 3\n",
      "Word: have -> Token 25\n"
     ]
    }
   ],
   "source": [
    "sentence_number = 100\n",
    "\n",
    "input_raw = X_train[sentence_number]\n",
    "input_token = X_train_token[sentence_number]\n",
    "\n",
    "for i in range(40):\n",
    "    print(f'Word: {input_raw[i]} -> Token {input_token[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3481c13",
   "metadata": {},
   "source": [
    "The dictionary that maps each word to a token can be accessed with __tokenizer.word_index__\n",
    "\n",
    "Let's add a __vocab_size__ variable that stores the number of different words (= tokens) in the train set. This is called the size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7d1b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 30419 different words in the train set\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "print(f'There are {vocab_size} different words in the train set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69359db",
   "metadata": {},
   "source": [
    "__X_train_token__ and __X_test_token__ contain sequences of different lengths.\n",
    "\n",
    "\n",
    "However, a neural network has to have a tensor as input. For this reason, we have to pad the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0832009",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post')\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59228c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
